---
title: CS372 Chapter 41 Reading Note
order: 1
thumbnailURL: /images/notes/swasey.jpeg
thumbnailAlt: Denison Swasey Chapel
description: Locality and The Fast File System
---

# Table of content

- [Table of content](#table-of-content)
- [The Problem: Poor Performance](#the-problem-poor-performance)
- [FFS: Disk Awareness Is The Solution](#ffs-disk-awareness-is-the-solution)
- [Organizing Structure: The Cylinder Group](#organizing-structure-the-cylinder-group)
- [Policies: How To Allocate Files and Directories](#policies-how-to-allocate-files-and-directories)
- [Measuring File Locality](#measuring-file-locality)
- [The Large-File Exception](#the-large-file-exception)
- [A Few Other Things About FFS](#a-few-other-things-about-ffs)

# The Problem: Poor Performance

Performance of the previous chapter FS is bad.

The main issue was that the FS treated the disk like it was a random-access memory; data was spread all over the place without regard to the fact that the medium holding the data was a disk, and thus had real and expensive positioning costs.

# FFS: Disk Awareness Is The Solution

The idea was to design the file system structures and allocation policies to be “disk aware” and thus improve performance.

Keeping the same interface to the file system (the same APIs, including open(), read(), write(), close(), and other file system calls) but changing the internal implementation.

# Organizing Structure: The Cylinder Group

The first step was to change the on-disk structures. FFS divides the disk into a number of cylinder groups. A single cylinder is a set of tracks on different surfaces of a hard drive that are the same distance from the center of the drive.

![figure 41.0](https://i.ibb.co/t2JCrCm/41-0.png)

Disks export a logical address space of blocks and hide details of their geometry from clients. Thus, modern file systems (such as Linux ext2, ext3, and ext4) organize the drive into block groups, each of which is a consecutive portion of the disk’s address space.

![figure 41.05](https://i.ibb.co/D44Vw1j/41-05.png)

The content of each block is similar to how VSFS consider the disk: a super block (but with replication), inode bitmap, data bitmap, inode table, and data blocks.

# Policies: How To Allocate Files and Directories

The basic mantra is simple: keep related stuff together.

Regarding the placement of directories. FFS attempts to balance out directories accross different groups, balance out number of free inodes in each group.

Regarding files, FFS make sure to allocate the data blocks of a file in the same group as its inode, thus preventing long seeks between inode and data. Also, FFS laces all files that are in the same directory in the cylinder group of the directory they are in (i.e if a user creates four files, `/a/b`, `/a/c`, `/a/d`, and `b/f`, FFS would try to place the first three near one another (same group) and the fourth far away (in some other group).)

# Measuring File Locality

We use SEER traces. If a file is open and re-opened in the next trace, the distance between the 2 opens are 0 (they are the same file). If we open `/dir/f` then open `dir/g`, their distance is 1 (in `/dir/g`, we navigate to `dir/f` by `../f` - number of slashes).

With the distance metric in mind, the figure below represent data in the real world of the distances:

![figure 41.1](https://i.ibb.co/vD7VXxJ/41-1.png)

For comparison, the graph also shows locality for a “Random” trace. The random trace was generated by selecting files from within an existing SEER trace in random order, and calculating the distance metric between these randomly-ordered accesses. As you can see, there is less namespace locality in the random traces, as expected.

However, because eventually every file shares a common ancestor (e.g., the root), there is some locality, and thus random is useful as a comparison point.

# The Large-File Exception

For large files, FFS does the following: after the first level of inode indexing is used up, FFS places the extra large chunk into other groups.

This will cause some performance issue, but it can be addressed if we choose the chunk size carefully. If the chunk size is large enough, the file system will spend most of its time transferring data from disk and just a (relatively) little time seeking between chunks of the block.

This process of reducing an overhead by doing more work per overhead paid is called **amortization** and is a common technique in computer systems.

The figure below shows the sweet spot for chunk size, allowing highest bandwidth possible while keeping chunk size reasonable, is 3.69MB (allowing 90% bandwidth).

![figure 41.2](https://i.ibb.co/kmRJdk4/40-2.png)

# A Few Other Things About FFS

Using 4KB blocks could introduce internal fragmentation if we have numerous files with sizes around 2KB (because we can only fit 1 in each block). To address this issue, we have sub-blocks, which are smaller blocks within blocks (512B). As the file grew in size, we keep allocation sub-blocks until it reachs the size of a block, then we copy those sub-blocks into a block, then free the sub-blocks. Sub-blocks are inefficient, so FFS just buffer the writes until the data reach 4KB in size.

FFS also introduce optimized disk layout. It would be a inefficient to do a sequential read that goes on the opposite direction of the disk (read block 0, go full rotation to read block 1).

The FFS disk placement:

![figure 41.3](https://i.ibb.co/NrKFYmg/41-3.png)

By skipping over every other block (in the example), FFS has enough time to request the next block before it went past the disk head. In fact, FFS was smart enough to figure out for a particular disk how many blocks it should skip in doing layout in order to avoid the extra rotations; this technique was called **parameterization**, as FFS would figure out the specific performance parameters of the disk and use those to decide on the exact staggered layout scheme.

This placement could cause us to have 50% bandwidth in sequential reads, but modern disk keep a cache of the entire track, minimizing the needs to get physical.

FFS also introduce long file names and symlink. Hard links are limited in that they both could not point to directories (for fear of introducing loops in the file system hierarchy) and that they can only point to files within the same volume (i.e., the inode number must still be meaningful). Symbolic links allow the user to create an “alias” to any other file or directory on a system and thus are much more flexible.

FFS also introduced an atomic rename() operation for renaming files.

---
title: CS372 Chapter 17 Reading Note
order: 1
thumbnailURL: /images/notes/swasey.jpeg
thumbnailAlt: Denison Swasey Chapel
description: Paging - Faster Translation
---

# Table of content

- [Table of content](#table-of-content)
- [The Problem, TLBs](#the-problem-tlbs)
- [TLB Basic Algorithm](#tlb-basic-algorithm)
- [Example: Accessing An Array](#example-accessing-an-array)
- [Who Handles The TLB Miss](#who-handles-the-tlb-miss)
- [TLB Contents: What’s In There](#tlb-contents-whats-in-there)
- [TLB Issue: Context Switches](#tlb-issue-context-switches)
- [Issue: Replacement Policy](#issue-replacement-policy)

# The Problem, TLBs

How can we speed up address translation, and generally avoid the extra memory reference that paging seems to require? What hardware support is required? What OS involvement is needed?

To speed address translation, we are going to add what is called a translation-lookaside buffer, or TLB.

A TLB is part of the chip’s memory-management unit (MMU), and is simply a hardware cache of popular virtual-to-physical address translations; thus, a better name would be an address-translation cache.

Upon each virtual memory reference, the hardware first checks the TLB to see if the desired translation is held therein; if so, the translation is performed (quickly) without having to consult the page table (which has all translations). Because of their tremendous performance impact, TLBs in a real sense make virtual memory possible.

# TLB Basic Algorithm

The figure below shows a rough sketch of how hardware might handle a virtual address translation, assuming a simple linear page table (i.e., the page table is an array) and a hardware-managed TLB (i.e., the hardware handles much of the responsibility of page table accesses.

![figure 19.1](https://i.ibb.co/1fM94mx/19-1.png)

The algorithm the hardware follows works like this:

1. Extract the virtual page number (VPN) from the virtual address (1)
2. Check if the TLB holds the translation for this VPN (2)
3. If it does, we have a TLB hit, which means the TLB holds the translation
4. Check protection bit (4)
5. We can now extract the page frame number (PFN) from the relevant TLB entry, concatenate that onto the offset from the original virtual address, and form the desired physical address (PA), and access memory (5-7)
6. If the CPU does not find the translation in the TLB (a TLB miss), the hardware accesses the page table to find the translation (11-12)
7. Assuming that the virtual memory reference generated by the process is valid and accessible (13-16)
8. Updates the TLB with the translation

Once the TLB is updated, the hardware retries the instruction; this time, the translation is found in the TLB, and the memory reference is processed quickly.

# Example: Accessing An Array

- Access a[0]: The CPU will see a load to virtual address 100. The hardware extracts the VPN from this (VPN=06), and uses that to check the TLB for a valid translation. Assuming this is the first time the program accesses the array, the result will be a TLB miss.
- Access a[1]: A TLB hit! Because the second element of the array is packed next to the first, it lives on the same page; because we’ve already accessed this page when accessing the first element of the array, the translation is already loaded into the TLB.
- Access a[2]: Hit for the same reason as a[1]
- The same rules repeats for a[3] - a[6], and a[7] - a[9].

![figure 19.2](https://i.ibb.co/7CbkWBy/19-2.png)

Thus, our TLB hit rate, which is the number of hits divided by the total number of accesses, is 70%. Although this is not too high (indeed, we desire hit rates that ap- proach 100%), it is non-zero, which may be a surprise.

Even though this is the first time the program accesses the array, the TLB improves performance due to spatial locality. The elements of the array are packed tightly into pages (i.e., they are close to one another in space), and thus only the first access to an element on a page yields a TLB miss.

One last point about TLB performance: if the program, soon after this loop completes, accesses the array again, we’d likely see an even bet- ter result, assuming that we have a big enough TLB to cache the needed translations.

In this case, the TLB hit rate would be high because of temporal locality, i.e., the quick re-referencing of memory items in time.

# Who Handles The TLB Miss

Two answers are possible: the hardware, or the software (OS).

The hardware would handle the TLB miss entirely. To do this, the hardware has to know exactly where the page tables are located in memory as well as their exact format; on a miss, the hardware would “walk” the page table, find the correct page-table entry and extract the desired translation, update the TLB with the translation, and retry the instruction.

More modern architectures have what is known as a software-managed TLB. On a TLB miss, the hardware simply raises an exception, which pauses the current instruction stream, raises the privilege level to kernel mode, and jumps to a trap handler.

This trap handler is code within the OS that is written with the express purpose of handling TLB misses. When run, the code will lookup the translation in the page table, use spe- cial “privileged” instructions to update the TLB, and return from the trap; at this point, the hardware retries the instruction (resulting in a TLB hit).

# TLB Contents: What’s In There

Let’s look at the contents of the hardware TLB in more detail. A typical TLB might have 32, 64, or 128 entries and be what is called fully associative. Any given translation can be anywhere in the TLB, and that the hardware will search the entire TLB in parallel to find the desired translation. A TLB entry might look contain the following:

```
VPN | PFC | other bits
```

In “other bits”, the TLB commonly has a **valid bit**, which says whether the entry has a valid translation or not. Also common are **protection bits**, which determine how a page can be accessed (as in the page table). For example, code pages might be marked read and execute, whereas heap pages might be marked read and write. There may also be a few other fields, including an **address-space identifier**, a **dirty bit**, and so forth.

# TLB Issue: Context Switches

The TLB contains virtual-to-physical translations that are only valid for the currently running process; these translations are not meaningful for other processes. As a result, when switching from one process to another, the hardware or OS (or both) must be careful to ensure that the about-to-be-run process does not accidentally use translations from some previously run process.

Example: when one process (P1) is running:

- Assumes the TLB might be caching translations that are valid for it, i.e., that come from P1’s page table.
- Assume for this example, that the 10th virtual page of P1 is mapped to physical frame 100.
- Assume another process (P2) exists, and the OS soon might decide to perform a context switch and run it.
- Assume here that the 10th virtual page of P2 is mapped to physical frame 170.

If entries for both processes were in the TLB, the contents of the TLB would be:

![context-switch-table](https://i.ibb.co/r3tH3BQ/context-switch-table.png)

In the TLB above, we clearly have a problem: VPN 10 translates to either PFN 100 (P1) or PFN 170 (P2), but the hardware can’t distinguish which entry is meant for which process. Thus, we need to do some more work in order for the TLB to correctly and efficiently support virtualiza- tion across multiple processes.

One approach is to simply flush the TLB on context switches, thus emptying it before running the next process. On a software-based system, this can be accomplished with an explicit (and privileged) hardware instruction (with a hardware-managed TLB, the flush could be enacted when the page-table base register is changed (note the OS must change the PTBR on a context switch anyhow). The flush operation simply sets all valid bits to 0, essentially clearing the contents of the TLB.

This approach will ensure no wrong translation in the TLB, but each time a process runs, it must incur TLB misses. If the OS switches between processes frequently, the overhead cost would be high.

To reduce this overhead, some systems add hardware support to en- able sharing of the TLB across context switches. In particular, some hard- ware systems provide an address space identifier (ASID) field in the TLB. You can think of the ASID as a process identifier (PID), but usually it has fewer bits (e.g., 8 bits for the ASID versus 32 bits for a PID)

Here is a depiction of a TLB with the added ASID field:

![context-switch-table-2](https://i.ibb.co/mNvLfdQ/context-switch-table-2.png)

Thus, with address-space identifiers, the TLB can hold translations from different processes at the same time without any confusion.

There are situtation where 2 processes share a page. In the example above, Process 1 is sharing physical page 101 with Process 2; P1 maps this page into the 10th page of its address space, whereas P2 maps it to the 50th page of its address space.

Sharing of code pages (in binaries, or shared libraries) is useful as it reduces the number of physical pages in use, thus reducing memory overheads.

# Issue: Replacement Policy

When we are installing a new entry in the TLB, we have to replace an old one, and thus the question: which one to replace?

One common approach is to evict the least-recently-used or LRU entry. LRU tries to take advantage of locality in the memory-reference stream, assuming it is likely that an entry that has not recently been used is a good candidate for eviction.

Another typical approach is to use a random pol- icy, which evicts a TLB mapping at random. Such a policy is useful due to its simplicity and ability to avoid corner-case behaviors; for example, a “reasonable” policy such as LRU behaves quite unreasonably when a program loops over n + 1 pages with a TLB of size n; in this case, LRU misses upon every access, whereas random does much better.
